{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMf/9Rz8xz5u0HTdHweipLu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olusegvn/Numerai/blob/main/Numerai%20Keras%20XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9BP3-z7tZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80285f41-f138-4a81-fd93-8820dcf93104"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from google.colab import drive\n",
        "import xgboost as xg\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5eIn38KEsD"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Numerai datasets\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqLmgKS7FYQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e47d00-7044-4a75-8f23-47c066f6c0a3"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "TOURNAMENT_NAME = \"kazutsugi\"\n",
        "TARGET_NAME = f\"target_{TOURNAMENT_NAME}\"\n",
        "PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n",
        "\n",
        "MODEL_FILE = Path(\"XGBoostClassifier\")\n",
        "\n",
        "\n",
        "\n",
        "# Submissions are scored by spearman correlation\n",
        "def correlation(predictions, targets):\n",
        "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
        "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
        "\n",
        "\n",
        "# convenience method for scoring\n",
        "def score(df):\n",
        "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])\n",
        "\n",
        "\n",
        "# Payout is just the score cliped at +/-25%\n",
        "def payout(scores):\n",
        "    return scores.clip(lower=-0.25, upper=0.25)\n",
        "\n",
        "\n",
        "# Read the csv file into a pandas Dataframe as float16 to save space\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        column_names = next(csv.reader(f))\n",
        "\n",
        "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
        "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
        "\n",
        "    # Memory constrained? Try this instead (slower, but more memory efficient)\n",
        "    # see https://forum.numer.ai/t/saving-memory-with-uint8-features/254\n",
        "    # dtypes = {f\"target_{TOURNAMENT_NAME}\": np.float16}\n",
        "    # to_uint8 = lambda x: np.uint8(float(x) * 4)\n",
        "    # converters = {x: to_uint8 for x in column_names if x.startswith('feature')}\n",
        "    # df = pd.read_csv(file_path, dtype=dtypes, converters=converters)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "functions used for advanced metrics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apNone\n",
        "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "records = pd.read_pickle('NUMERAI_RECORDS.pkl')\n",
        "print(records)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             model  train_correlations_mean  \\\n",
            "0                                xg.XGBRegressor()                 0.081611   \n",
            "0                    xg.XGBRegressor(max_depth=10)                 0.130500   \n",
            "0                xg.XGBRegressor(n_estimators=500)                 0.130500   \n",
            "0                xg.XGBRegressor(n_estimators=700)                 0.146581   \n",
            "0                    xg.XGBRegressor(max_depth=10)                 0.760064   \n",
            "..                                             ...                      ...   \n",
            "0                xg.XGBRegressor(n_estimators=670)                 0.144520   \n",
            "0                xg.XGBRegressor(n_estimators=692)                 0.146104   \n",
            "0   xg.XGBRegressor(n_estimators=692, max_depth=5)                 0.361358   \n",
            "0   xg.XGBRegressor(n_estimators=692, max_depth=4)                 0.232260   \n",
            "0   xg.XGBRegressor(n_estimators=692, max_depth=5)                 0.247150   \n",
            "\n",
            "    train_correlations_std  train_payout  validation_correlations_mean  \\\n",
            "0                 0.037205      0.081611                      0.024590   \n",
            "0                 0.031651      0.130500                      0.028433   \n",
            "0                 0.031651      0.130500                      0.028433   \n",
            "0                 0.029558      0.146581                      0.029011   \n",
            "0                 0.016725      0.250000                      0.017275   \n",
            "..                     ...           ...                           ...   \n",
            "0                 0.029721      0.144520                      0.028978   \n",
            "0                 0.029578      0.146104                      0.029136   \n",
            "0                 0.024177      0.250000                      0.022634   \n",
            "0                 0.027527      0.228316                      0.026682   \n",
            "0                 0.026246      0.238212                      0.026622   \n",
            "\n",
            "    validation_correlations_std  validation_payout  validation_sharpe  \\\n",
            "0                      0.032413           0.024590           0.758665   \n",
            "0                      0.028650           0.028433           0.992438   \n",
            "0                      0.028650           0.028433           0.992438   \n",
            "0                      0.028070           0.029011           1.033530   \n",
            "0                      0.024517           0.017275           0.704596   \n",
            "..                          ...                ...                ...   \n",
            "0                      0.027983           0.028978           1.035541   \n",
            "0                      0.028068           0.029136           1.038059   \n",
            "0                      0.027660           0.022634           0.818307   \n",
            "0                      0.025887           0.026682           1.030695   \n",
            "0                      0.026438           0.026622           1.006957   \n",
            "\n",
            "    max_drawdown  max_feature_exposure  feature_neutral_mean  MMC_mean  \\\n",
            "0      -0.068834              0.321752              0.017334 -0.000651   \n",
            "0      -0.071341              0.270339              0.023381  0.002677   \n",
            "0      -0.071341              0.270339              0.023381  0.002677   \n",
            "0      -0.065506              0.268950              0.024296  0.004001   \n",
            "0      -0.051496              0.230871              0.012428  0.003409   \n",
            "..           ...                   ...                   ...       ...   \n",
            "0      -0.066020              0.265605              0.024267  0.003854   \n",
            "0      -0.065685              0.268525              0.024429  0.004130   \n",
            "0      -0.066687              0.256818              0.017834  0.002006   \n",
            "0      -0.060673              0.251994              0.021689  0.003651   \n",
            "0      -0.057801              0.255714              0.022083  0.004119   \n",
            "\n",
            "    corr_plus_mmc_sharpe  corr_plus_mmc_sharpe_diff  corr_with_example_preds  \n",
            "0               0.592845                  -0.165820                 0.869042  \n",
            "0               0.991883                  -0.000555                 0.907129  \n",
            "0               0.991883                  -0.000555                 0.907129  \n",
            "0               1.058246                   0.024716                 0.888952  \n",
            "0               0.516827                  -0.187769                 0.496990  \n",
            "..                   ...                        ...                      ...  \n",
            "0               1.064516                   0.028975                 0.892277  \n",
            "0               1.068368                   0.030308                 0.889766  \n",
            "0               0.639497                  -0.178811                 0.726409  \n",
            "0               1.039526                   0.008831                 0.818954  \n",
            "0               0.976772                  -0.030186                 0.803534  \n",
            "\n",
            "[14 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfUGiCSFI7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "33f0bb46-c698-44d8-ab52-58689745233c"
      },
      "source": [
        "def main():\n",
        "    model = xg.XGBRegressor(learning_rate=0.001, n_estimators=5000, verbosity=0, objective='reg:squarederror', n_jobs=-1)\n",
        "    model_string = \"xg.XGBRegressor(learning_rate=0.01, n_estimators=692, verbosity=0, objective='reg:squarederror', n_jobs=-1)\"\n",
        "    print(\"Loading data...\")\n",
        "    # The training data is used to train your model how to predict the targets.\n",
        "    training_data = read_csv(\"numerai_training_data.csv\")\n",
        "    # The tournament data is the data that Numerai uses to evaluate your model.\n",
        "    tournament_data = read_csv(\"numerai_tournament_data.csv\")\n",
        "    feature_names = [\n",
        "        f for f in training_data.columns if f.startswith(\"feature\")\n",
        "    ]\n",
        "    print(f\"Loaded {len(feature_names)} features\")\n",
        "\n",
        "    # This is the model that generates the included example predictions file.\n",
        "    # Taking too long? Set learning_rate=0.1 and n_estimators=200 to make this run faster.\n",
        "    # Remember to delete example_model.xgb if you change any of the parameters below.\n",
        "\n",
        "    try:\n",
        "        pickle.loads(MODEL_FILE)\n",
        "        print(\"loaded pre-trained model...\")\n",
        "    except:\n",
        "        print(\"Training model...\")\n",
        "    model.fit(np.array(training_data[feature_names]), training_data[TARGET_NAME])\n",
        "    pickle.dumps(MODEL_FILE)\n",
        "    # Generate predictions on both training and tournament data\n",
        "    print(\"Generating predictions...\")\n",
        "    training_data[PREDICTION_NAME] = model.predict(np.array(training_data[feature_names]))\n",
        "    tournament_data[PREDICTION_NAME] = model.predict(np.array(tournament_data[feature_names]))\n",
        "\n",
        "    # Check the per-era correlations on the training set (in sample)\n",
        "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std()}\")\n",
        "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
        "\n",
        "    \"\"\"Validation Metrics\"\"\"\n",
        "    # Check the per-era correlations on the validation set (out of sample)\n",
        "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
        "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
        "          f\"std {validation_correlations.std()}\")\n",
        "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
        "\n",
        "    # Check the \"sharpe\" ratio on the validation set\n",
        "    validation_sharpe = validation_correlations.mean() / validation_correlations.std()\n",
        "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
        "\n",
        "    print(\"checking max drawdown...\")\n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
        "                                                                  min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    print(f\"max drawdown: {max_drawdown}\")\n",
        "\n",
        "    # Check the feature exposure of your validation predictions\n",
        "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
        "                                                             axis=0)\n",
        "    max_feature_exposure = np.max(np.abs(feature_exposures))\n",
        "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
        "\n",
        "    # Check feature neutral mean\n",
        "    print(\"Calculating feature neutral mean...\")\n",
        "    feature_neutral_mean = get_feature_neutral_mean(validation_data)\n",
        "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
        "\n",
        "    # Load example preds to get MMC metrics\n",
        "    example_preds = pd.read_csv(\"example_predictions_target_kazutsugi.csv\").set_index(\"id\")[\"prediction_kazutsugi\"]\n",
        "    validation_example_preds = example_preds.loc[validation_data.index]\n",
        "    validation_data[\"ExamplePreds\"] = validation_example_preds\n",
        "\n",
        "    print(\"calculating MMC stats...\")\n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in validation_data.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[\"ExamplePreds\"])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
        "\n",
        "    print(\n",
        "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
        "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
        "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
        "    )\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(validation_example_preds.rank(pct=True, method=\"first\"),\n",
        "                                          validation_data[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(f\"Corr with example preds: {corr_with_example_preds}\")\n",
        "\n",
        "    # Save predictions as a CSV and upload to https://numer.ai\n",
        "    tournament_data[PREDICTION_NAME].to_csv(TOURNAMENT_NAME + \"_submission.csv\")\n",
        "\n",
        "    # Record model performance\n",
        "    print('recording model performance')\n",
        "    record = [{'model': model_string, 'train_correlations_mean':train_correlations.mean(), 'train_correlations_std':train_correlations.std(), 'train_payout':payout(train_correlations).mean(), 'validation_correlations_mean':validation_correlations.mean(), 'validation_correlations_std':validation_correlations.std(), 'validation_payout': payout(validation_correlations).mean(), 'validation_sharpe': validation_sharpe, 'max_drawdown': max_drawdown, 'max_feature_exposure': max_feature_exposure, 'feature_neutral_mean': feature_neutral_mean, 'MMC_mean': val_mmc_mean, 'corr_plus_mmc_sharpe': corr_plus_mmc_sharpe, 'corr_plus_mmc_sharpe_diff': corr_plus_mmc_sharpe_diff, 'corr_with_example_preds': corr_with_example_preds  }]\n",
        "    if os.path.isfile('NUMERAI_RECORDS.pkl'):\n",
        "      records = pd.read_pickle('NUMERAI_RECORDS.pkl')\n",
        "      records = records.append(record)\n",
        "    else:\n",
        "      records = pd.DataFrame(record)\n",
        "    records.to_pickle('NUMERAI_RECORDS.pkl')\n",
        "\n",
        "main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-731b01eabc0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NUMERAI_RECORDS.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-731b01eabc0a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numerai_training_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# The tournament data is the data that Numerai uses to evaluate your model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtournament_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numerai_tournament_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     feature_names = [\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b5dbf77bc6a4>\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Memory constrained? Try this instead (slower, but more memory efficient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scpEq4YKzn1E"
      },
      "source": [
        "8 x 800 epochs, steady increase until now. CV of previous ; 0.005"
      ]
    }
  ]
}