{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFmHS9dJ2ndFUlGGNH2uSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olusegvn/Numerai/blob/main/Numerai%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5Di0Efbc8G",
        "outputId": "603fc1b5-6c8d-4513-deb1-d2e7facd10db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install keras_utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_utils\n",
            "  Downloading https://files.pythonhosted.org/packages/31/a2/8be2aee1c8cd388e83d447556c2c84a396944c8bad93d710c5e757f8e98e/keras-utils-1.0.13.tar.gz\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_utils) (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.1.5->keras_utils) (1.15.0)\n",
            "Building wheels for collected packages: keras-utils\n",
            "  Building wheel for keras-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-utils: filename=keras_utils-1.0.13-cp36-none-any.whl size=2656 sha256=f00d4b491be2831b35047b865465c7472a710412cc34e460bf69d27df5b5ecd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/25/27/7707005c1cb27e1ffc8277b004ac295e34767b02b44d73d6be\n",
            "Successfully built keras-utils\n",
            "Installing collected packages: keras-utils\n",
            "Successfully installed keras-utils-1.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9BP3-z7tZy",
        "outputId": "b8e60216-ee84-44cf-87ef-1bd029ee1f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import keras_utils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We're using TF 2.3.0\n",
            "We are using Keras 2.4.3\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5eIn38KEsD"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Numerai datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqLmgKS7FYQL"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "TOURNAMENT_NAME = \"kazutsugi\"\n",
        "TARGET_NAME = f\"target_{TOURNAMENT_NAME}\"\n",
        "PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n",
        "\n",
        "MODEL_FILE = Path(\"kerasmodel1502Regularize\")\n",
        "\n",
        "\n",
        "\n",
        "# Submissions are scored by spearman correlation\n",
        "def correlation(predictions, targets):\n",
        "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
        "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
        "\n",
        "\n",
        "# convenience method for scoring\n",
        "def score(df):\n",
        "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])\n",
        "\n",
        "\n",
        "# Payout is just the score cliped at +/-25%\n",
        "def payout(scores):\n",
        "    return scores.clip(lower=-0.25, upper=0.25)\n",
        "\n",
        "\n",
        "# Read the csv file into a pandas Dataframe as float16 to save space\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        column_names = next(csv.reader(f))\n",
        "\n",
        "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
        "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
        "\n",
        "    # Memory constrained? Try this instead (slower, but more memory efficient)\n",
        "    # see https://forum.numer.ai/t/saving-memory-with-uint8-features/254\n",
        "    # dtypes = {f\"target_{TOURNAMENT_NAME}\": np.float16}\n",
        "    # to_uint8 = lambda x: np.uint8(float(x) * 4)\n",
        "    # converters = {x: to_uint8 for x in column_names if x.startswith('feature')}\n",
        "    # df = pd.read_csv(file_path, dtype=dtypes, converters=converters)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "functions used for advanced metrics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVzKBpHIDq54"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfUGiCSFI7g",
        "outputId": "42aac012-22cd-43fa-f2f1-b7254916d1a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "def main():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(155, kernel_regularizer=\"l2\"))\n",
        "    model.add(Activation('swish'))\n",
        "    model.add(Dense(1, kernel_regularizer=\"l2\"))\n",
        "    model.add(Activation('swish'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer=\"adam\",\n",
        "    )\n",
        "    print(\"Loading data...\")\n",
        "    # The training data is used to train your model how to predict the targets.\n",
        "    training_data = read_csv(\"numerai_training_data.csv\")\n",
        "    # The tournament data is the data that Numerai uses to evaluate your model.\n",
        "    tournament_data = read_csv(\"numerai_tournament_data.csv\")\n",
        "\n",
        "    feature_names = [\n",
        "        f for f in training_data.columns if f.startswith(\"feature\")\n",
        "    ]\n",
        "    print(f\"Loaded {len(feature_names)} features\")\n",
        "\n",
        "    # This is the model that generates the included example predictions file.\n",
        "    # Taking too long? Set learning_rate=0.1 and n_estimators=200 to make this run faster.\n",
        "    # Remember to delete example_model.xgb if you change any of the parameters below.\n",
        "\n",
        "    try:\n",
        "        # model = keras.models.load_model(MODEL_FILE)\n",
        "        print(\"loaded pre-trained model...\")\n",
        "    except:\n",
        "        print(\"Training model...\")\n",
        "    model.fit(training_data[feature_names], training_data[TARGET_NAME], epochs=800, batch_size=1000, verbose=2)\n",
        "    model.save(MODEL_FILE)\n",
        "\n",
        "    # Generate predictions on both training and tournament data\n",
        "    print(\"Generating predictions...\")\n",
        "    training_data[PREDICTION_NAME] = model.predict(training_data[feature_names])\n",
        "    tournament_data[PREDICTION_NAME] = model.predict(tournament_data[feature_names])\n",
        "\n",
        "    # Check the per-era correlations on the training set (in sample)\n",
        "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std()}\")\n",
        "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
        "\n",
        "    \"\"\"Validation Metrics\"\"\"\n",
        "    # Check the per-era correlations on the validation set (out of sample)\n",
        "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
        "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
        "          f\"std {validation_correlations.std()}\")\n",
        "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
        "\n",
        "    # Check the \"sharpe\" ratio on the validation set\n",
        "    validation_sharpe = validation_correlations.mean() / validation_correlations.std()\n",
        "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
        "\n",
        "    print(\"checking max drawdown...\")\n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
        "                                                                  min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    print(f\"max drawdown: {max_drawdown}\")\n",
        "\n",
        "    # Check the feature exposure of your validation predictions\n",
        "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
        "                                                             axis=0)\n",
        "    max_feature_exposure = np.max(np.abs(feature_exposures))\n",
        "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
        "\n",
        "    # Check feature neutral mean\n",
        "    print(\"Calculating feature neutral mean...\")\n",
        "    feature_neutral_mean = get_feature_neutral_mean(validation_data)\n",
        "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
        "\n",
        "    # Load example preds to get MMC metrics\n",
        "    example_preds = pd.read_csv(\"example_predictions_target_kazutsugi.csv\").set_index(\"id\")[\"prediction_kazutsugi\"]\n",
        "    validation_example_preds = example_preds.loc[validation_data.index]\n",
        "    validation_data[\"ExamplePreds\"] = validation_example_preds\n",
        "\n",
        "    print(\"calculating MMC stats...\")\n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in validation_data.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[\"ExamplePreds\"])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
        "\n",
        "    print(\n",
        "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
        "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
        "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
        "    )\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(validation_example_preds.rank(pct=True, method=\"first\"),\n",
        "                                          validation_data[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(f\"Corr with example preds: {corr_with_example_preds}\")\n",
        "\n",
        "    # Save predictions as a CSV and upload to https://numer.ai\n",
        "    tournament_data[PREDICTION_NAME].to_csv(TOURNAMENT_NAME + \"_submission.csv\")\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}