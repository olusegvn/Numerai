{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1dVf51UOKSQBsBgiKMGTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olusegvn/Numerai/blob/main/Numerai%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5Di0Efbc8G",
        "outputId": "c79d28bd-9a3a-4a99-bc52-62a239abf750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install keras_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_utils\n",
            "  Downloading https://files.pythonhosted.org/packages/31/a2/8be2aee1c8cd388e83d447556c2c84a396944c8bad93d710c5e757f8e98e/keras-utils-1.0.13.tar.gz\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_utils) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.1.5->keras_utils) (1.15.0)\n",
            "Building wheels for collected packages: keras-utils\n",
            "  Building wheel for keras-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-utils: filename=keras_utils-1.0.13-cp36-none-any.whl size=2656 sha256=403827c039cee2d29157eef45270cc02915908a172fb64b82c223e219ccd6f22\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/25/27/7707005c1cb27e1ffc8277b004ac295e34767b02b44d73d6be\n",
            "Successfully built keras-utils\n",
            "Installing collected packages: keras-utils\n",
            "Successfully installed keras-utils-1.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9BP3-z7tZy",
        "outputId": "ad1ceff2-9488-493a-88ae-34036a7647e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import keras_utils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We're using TF 2.3.0\n",
            "We are using Keras 2.4.3\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5eIn38KEsD"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Numerai datasets\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqLmgKS7FYQL"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "TOURNAMENT_NAME = \"kazutsugi\"\n",
        "TARGET_NAME = f\"target_{TOURNAMENT_NAME}\"\n",
        "PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n",
        "\n",
        "MODEL_FILE = Path(\"keras_model1_1122\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Submissions are scored by spearman correlation\n",
        "def correlation(predictions, targets):\n",
        "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
        "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
        "\n",
        "\n",
        "# convenience method for scoring\n",
        "def score(df):\n",
        "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])\n",
        "\n",
        "\n",
        "# Payout is just the score cliped at +/-25%\n",
        "def payout(scores):\n",
        "    return scores.clip(lower=-0.25, upper=0.25)\n",
        "\n",
        "\n",
        "# Read the csv file into a pandas Dataframe as float16 to save space\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        column_names = next(csv.reader(f))\n",
        "\n",
        "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
        "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
        "\n",
        "    # Memory constrained? Try this instead (slower, but more memory efficient)\n",
        "    # see https://forum.numer.ai/t/saving-memory-with-uint8-features/254\n",
        "    # dtypes = {f\"target_{TOURNAMENT_NAME}\": np.float16}\n",
        "    # to_uint8 = lambda x: np.uint8(float(x) * 4)\n",
        "    # converters = {x: to_uint8 for x in column_names if x.startswith('feature')}\n",
        "    # df = pd.read_csv(file_path, dtype=dtypes, converters=converters)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "functions used for advanced metrics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVzKBpHIDq54"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfUGiCSFI7g",
        "outputId": "c1748849-4361-4b09-fe77-e9e9dee46169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "def main():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(155))\n",
        "    model.add(Activation('swish'))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('swish'))\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.CategoricalHinge(),\n",
        "        optimizer='adam',\n",
        "    )\n",
        "    print(\"Loading data...\")\n",
        "    # The training data is used to train your model how to predict the targets.\n",
        "    training_data = read_csv(\"numerai_training_data.csv\")\n",
        "    # The tournament data is the data that Numerai uses to evaluate your model.\n",
        "    tournament_data = read_csv(\"numerai_tournament_data.csv\")\n",
        "\n",
        "    feature_names = [\n",
        "        f for f in training_data.columns if f.startswith(\"feature\")\n",
        "    ]\n",
        "    print(f\"Loaded {len(feature_names)} features\")\n",
        "\n",
        "    # This is the model that generates the included example predictions file.\n",
        "    # Taking too long? Set learning_rate=0.1 and n_estimators=200 to make this run faster.\n",
        "    # Remember to delete example_model.xgb if you change any of the parameters below.\n",
        "\n",
        "    try:\n",
        "        model = keras.models.load_model(MODEL_FILE)\n",
        "        print(\"loaded pre-trained model...\")\n",
        "    except:\n",
        "        print(\"Training model...\")\n",
        "    model.fit(training_data[feature_names], training_data[TARGET_NAME], epochs=800, batch_size=1000, verbose=2)\n",
        "    model.save(MODEL_FILE)\n",
        "\n",
        "    # Generate predictions on both training and tournament data\n",
        "    print(\"Generating predictions...\")\n",
        "    training_data[PREDICTION_NAME] = model.predict(training_data[feature_names])\n",
        "    tournament_data[PREDICTION_NAME] = model.predict(tournament_data[feature_names])\n",
        "\n",
        "    # Check the per-era correlations on the training set (in sample)\n",
        "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std()}\")\n",
        "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
        "\n",
        "    \"\"\"Validation Metrics\"\"\"\n",
        "    # Check the per-era correlations on the validation set (out of sample)\n",
        "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
        "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
        "          f\"std {validation_correlations.std()}\")\n",
        "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
        "\n",
        "    # Check the \"sharpe\" ratio on the validation set\n",
        "    validation_sharpe = validation_correlations.mean() / validation_correlations.std()\n",
        "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
        "\n",
        "    print(\"checking max drawdown...\")\n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
        "                                                                  min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    print(f\"max drawdown: {max_drawdown}\")\n",
        "\n",
        "    # Check the feature exposure of your validation predictions\n",
        "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
        "                                                             axis=0)\n",
        "    max_feature_exposure = np.max(np.abs(feature_exposures))\n",
        "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
        "\n",
        "    # Check feature neutral mean\n",
        "    print(\"Calculating feature neutral mean...\")\n",
        "    feature_neutral_mean = get_feature_neutral_mean(validation_data)\n",
        "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
        "\n",
        "    # Load example preds to get MMC metrics\n",
        "    example_preds = pd.read_csv(\"example_predictions_target_kazutsugi.csv\").set_index(\"id\")[\"prediction_kazutsugi\"]\n",
        "    validation_example_preds = example_preds.loc[validation_data.index]\n",
        "    validation_data[\"ExamplePreds\"] = validation_example_preds\n",
        "\n",
        "    print(\"calculating MMC stats...\")\n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in validation_data.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[\"ExamplePreds\"])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
        "\n",
        "    print(\n",
        "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
        "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
        "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
        "    )\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(validation_example_preds.rank(pct=True, method=\"first\"),\n",
        "                                          validation_data[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(f\"Corr with example preds: {corr_with_example_preds}\")\n",
        "\n",
        "    # Save predictions as a CSV and upload to https://numer.ai\n",
        "    tournament_data[PREDICTION_NAME].to_csv(TOURNAMENT_NAME + \"_submission.csv\")\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loaded 310 features\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_420151) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_420151) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_420375) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_420375) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_activation_5_layer_call_and_return_conditional_losses_420236) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_activation_5_layer_call_and_return_conditional_losses_420493) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_420404) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_420404) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_activation_4_layer_call_and_return_conditional_losses_420459) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_activation_4_layer_call_and_return_conditional_losses_420192) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "loaded pre-trained model...\n",
            "Epoch 1/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9091\n",
            "Epoch 2/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9095\n",
            "Epoch 3/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9091\n",
            "Epoch 4/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9091\n",
            "Epoch 5/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 6/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9089\n",
            "Epoch 7/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 8/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9092\n",
            "Epoch 9/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9089\n",
            "Epoch 10/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 11/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 12/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9088\n",
            "Epoch 13/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 14/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9089\n",
            "Epoch 15/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9087\n",
            "Epoch 16/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9085\n",
            "Epoch 17/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9090\n",
            "Epoch 18/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 19/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9087\n",
            "Epoch 20/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9087\n",
            "Epoch 21/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9089\n",
            "Epoch 22/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9086\n",
            "Epoch 23/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9090\n",
            "Epoch 24/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9086\n",
            "Epoch 25/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9088\n",
            "Epoch 26/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9084\n",
            "Epoch 27/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9088\n",
            "Epoch 28/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9086\n",
            "Epoch 29/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9088\n",
            "Epoch 30/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9087\n",
            "Epoch 31/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9090\n",
            "Epoch 32/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9087\n",
            "Epoch 33/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9085\n",
            "Epoch 34/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9087\n",
            "Epoch 35/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9084\n",
            "Epoch 36/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9083\n",
            "Epoch 37/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9084\n",
            "Epoch 38/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9086\n",
            "Epoch 39/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9089\n",
            "Epoch 40/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9083\n",
            "Epoch 41/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9083\n",
            "Epoch 42/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9082\n",
            "Epoch 43/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9085\n",
            "Epoch 44/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9083\n",
            "Epoch 45/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9084\n",
            "Epoch 46/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9082\n",
            "Epoch 47/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9083\n",
            "Epoch 48/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9085\n",
            "Epoch 49/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9085\n",
            "Epoch 50/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 51/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9082\n",
            "Epoch 52/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9083\n",
            "Epoch 53/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9084\n",
            "Epoch 54/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9083\n",
            "Epoch 55/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 56/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9086\n",
            "Epoch 57/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9082\n",
            "Epoch 58/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9079\n",
            "Epoch 59/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9079\n",
            "Epoch 60/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9083\n",
            "Epoch 61/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 62/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9080\n",
            "Epoch 63/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9079\n",
            "Epoch 64/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 65/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9082\n",
            "Epoch 66/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 67/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9082\n",
            "Epoch 68/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9082\n",
            "Epoch 69/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 70/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 71/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 72/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9079\n",
            "Epoch 73/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9081\n",
            "Epoch 74/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 75/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 76/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 77/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 78/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9079\n",
            "Epoch 79/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 80/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 81/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 82/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9080\n",
            "Epoch 83/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9081\n",
            "Epoch 84/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 85/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9079\n",
            "Epoch 86/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 87/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9076\n",
            "Epoch 88/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9080\n",
            "Epoch 89/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 90/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 91/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9079\n",
            "Epoch 92/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 93/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 94/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 95/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 96/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 97/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9078\n",
            "Epoch 98/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 99/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 100/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9078\n",
            "Epoch 101/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 102/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 103/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 104/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 105/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 106/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 107/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 108/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 109/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9073\n",
            "Epoch 110/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9075\n",
            "Epoch 111/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 112/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9077\n",
            "Epoch 113/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 114/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 115/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 116/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 117/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9072\n",
            "Epoch 118/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 119/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9074\n",
            "Epoch 120/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 121/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 122/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 123/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 124/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9072\n",
            "Epoch 125/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9072\n",
            "Epoch 126/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9074\n",
            "Epoch 127/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 128/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 129/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 130/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9072\n",
            "Epoch 131/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9071\n",
            "Epoch 132/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9069\n",
            "Epoch 133/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9071\n",
            "Epoch 134/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9071\n",
            "Epoch 135/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 136/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 137/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 138/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9071\n",
            "Epoch 139/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9073\n",
            "Epoch 140/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9069\n",
            "Epoch 141/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 142/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 143/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 144/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 145/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 146/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9071\n",
            "Epoch 147/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9069\n",
            "Epoch 148/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9067\n",
            "Epoch 149/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9069\n",
            "Epoch 150/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 151/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 152/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9070\n",
            "Epoch 153/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 154/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9069\n",
            "Epoch 155/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9065\n",
            "Epoch 156/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 157/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 158/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 159/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9065\n",
            "Epoch 160/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 161/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9069\n",
            "Epoch 162/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9067\n",
            "Epoch 163/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 164/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 165/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 166/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9066\n",
            "Epoch 167/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9065\n",
            "Epoch 168/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 169/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9062\n",
            "Epoch 170/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 171/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9068\n",
            "Epoch 172/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 173/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 174/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 175/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9063\n",
            "Epoch 176/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9065\n",
            "Epoch 177/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9065\n",
            "Epoch 178/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 179/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9063\n",
            "Epoch 180/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9064\n",
            "Epoch 181/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 182/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 183/800\n",
            "502/502 [==============================] - 2s 3ms/step - loss: 0.9063\n",
            "Epoch 184/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9063\n",
            "Epoch 185/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 186/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 187/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9066\n",
            "Epoch 188/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 189/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9062\n",
            "Epoch 190/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 191/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 192/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9061\n",
            "Epoch 193/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 194/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 195/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9061\n",
            "Epoch 196/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 197/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9063\n",
            "Epoch 198/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9067\n",
            "Epoch 199/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 200/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9064\n",
            "Epoch 201/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9063\n",
            "Epoch 202/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 203/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9061\n",
            "Epoch 204/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 205/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 206/800\n",
            "502/502 [==============================] - 4s 8ms/step - loss: 0.9063\n",
            "Epoch 207/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 208/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 209/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 210/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 211/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 212/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9062\n",
            "Epoch 213/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 214/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9061\n",
            "Epoch 215/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 216/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 217/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 218/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 219/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 220/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9062\n",
            "Epoch 221/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 222/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 223/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 224/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 225/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9059\n",
            "Epoch 226/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 227/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 228/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 229/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9060\n",
            "Epoch 230/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 231/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 232/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 233/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 234/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 235/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 236/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 237/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 238/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 239/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9055\n",
            "Epoch 240/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 241/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 242/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9055\n",
            "Epoch 243/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 244/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 245/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 246/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9058\n",
            "Epoch 247/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 248/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 249/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 250/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 251/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9055\n",
            "Epoch 252/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9055\n",
            "Epoch 253/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 254/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 255/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 256/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 257/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9057\n",
            "Epoch 258/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 259/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 260/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 261/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 262/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9056\n",
            "Epoch 263/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 264/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 265/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 266/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9055\n",
            "Epoch 267/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 268/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 269/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 270/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 271/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 272/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 273/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 274/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 275/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 276/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 277/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9053\n",
            "Epoch 278/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 279/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 280/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 281/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 282/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 283/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 284/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 285/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 286/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 287/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9054\n",
            "Epoch 288/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 289/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 290/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 291/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 292/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 293/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 294/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 295/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 296/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 297/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 298/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9052\n",
            "Epoch 299/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 300/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 301/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 302/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 303/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 304/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 305/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 306/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 307/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 308/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 309/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 310/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 311/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 312/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 313/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 314/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 315/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 316/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 317/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 318/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9046\n",
            "Epoch 319/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9050\n",
            "Epoch 320/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 321/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9046\n",
            "Epoch 322/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 323/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 324/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9046\n",
            "Epoch 325/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 326/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 327/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 328/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 329/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9051\n",
            "Epoch 330/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 331/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 332/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 333/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 334/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9049\n",
            "Epoch 335/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 336/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 337/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 338/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 339/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 340/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 341/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 342/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 343/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9048\n",
            "Epoch 344/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 345/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 346/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 347/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 348/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9046\n",
            "Epoch 349/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 350/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 351/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 352/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 353/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 354/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 355/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 356/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 357/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 358/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 359/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 360/800\n",
            "502/502 [==============================] - 4s 8ms/step - loss: 0.9041\n",
            "Epoch 361/800\n",
            "502/502 [==============================] - 3s 7ms/step - loss: 0.9043\n",
            "Epoch 362/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 363/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9044\n",
            "Epoch 364/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 365/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 366/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 367/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 368/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 369/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9045\n",
            "Epoch 370/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 371/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 372/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 373/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 374/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 375/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 376/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 377/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 378/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 379/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 380/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 381/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 382/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 383/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9047\n",
            "Epoch 384/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 385/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 386/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 387/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 388/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 389/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 390/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 391/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 392/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 393/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 394/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 395/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 396/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9042\n",
            "Epoch 397/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 398/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 399/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 400/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 401/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 402/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 403/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 404/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 405/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9041\n",
            "Epoch 406/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 407/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 408/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 409/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 410/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 411/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 412/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9043\n",
            "Epoch 413/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 414/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 415/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 416/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 417/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 418/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9039\n",
            "Epoch 419/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 420/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 421/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 422/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 423/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 424/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 425/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 426/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 427/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 428/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 429/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 430/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 431/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9040\n",
            "Epoch 432/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9038\n",
            "Epoch 433/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 434/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 435/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 436/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 437/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 438/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 439/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 440/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9036\n",
            "Epoch 441/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 442/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 443/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9033\n",
            "Epoch 444/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9033\n",
            "Epoch 445/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 446/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9033\n",
            "Epoch 447/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9037\n",
            "Epoch 448/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9034\n",
            "Epoch 449/800\n",
            "502/502 [==============================] - 2s 4ms/step - loss: 0.9035\n",
            "Epoch 450/800\n",
            "108/502 [=====>........................] - ETA: 1s - loss: 0.9034"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}