{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqRN1Pjn7lnkFdfXShUnSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olusegvn/Numerai/blob/main/Numerai%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5Di0Efbc8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a2c63b-86e0-44c8-c614-d6e310103f3b"
      },
      "source": [
        "! pip install keras_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_utils\n",
            "  Downloading https://files.pythonhosted.org/packages/31/a2/8be2aee1c8cd388e83d447556c2c84a396944c8bad93d710c5e757f8e98e/keras-utils-1.0.13.tar.gz\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_utils) (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_utils) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.1.5->keras_utils) (1.15.0)\n",
            "Building wheels for collected packages: keras-utils\n",
            "  Building wheel for keras-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-utils: filename=keras_utils-1.0.13-cp36-none-any.whl size=2656 sha256=b31410f30222c738744e5cb68cbe4c5224051d9cf8fe17ae291bade9f38b994a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/25/27/7707005c1cb27e1ffc8277b004ac295e34767b02b44d73d6be\n",
            "Successfully built keras-utils\n",
            "Installing collected packages: keras-utils\n",
            "Successfully installed keras-utils-1.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9BP3-z7tZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561457fa-1b85-4d22-f700-c44166d99023"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import keras_utils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We're using TF 2.3.0\n",
            "We are using Keras 2.4.3\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5eIn38KEsD"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Numerai datasets\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqLmgKS7FYQL"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "TOURNAMENT_NAME = \"kazutsugi\"\n",
        "TARGET_NAME = f\"target_{TOURNAMENT_NAME}\"\n",
        "PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n",
        "\n",
        "MODEL_FILE = Path(\"kerasmodel8layersDropout\")\n",
        "\n",
        "\n",
        "\n",
        "# Submissions are scored by spearman correlation\n",
        "def correlation(predictions, targets):\n",
        "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
        "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
        "\n",
        "\n",
        "# convenience method for scoring\n",
        "def score(df):\n",
        "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])\n",
        "\n",
        "\n",
        "# Payout is just the score cliped at +/-25%\n",
        "def payout(scores):\n",
        "    return scores.clip(lower=-0.25, upper=0.25)\n",
        "\n",
        "\n",
        "# Read the csv file into a pandas Dataframe as float16 to save space\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        column_names = next(csv.reader(f))\n",
        "\n",
        "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
        "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
        "\n",
        "    # Memory constrained? Try this instead (slower, but more memory efficient)\n",
        "    # see https://forum.numer.ai/t/saving-memory-with-uint8-features/254\n",
        "    # dtypes = {f\"target_{TOURNAMENT_NAME}\": np.float16}\n",
        "    # to_uint8 = lambda x: np.uint8(float(x) * 4)\n",
        "    # converters = {x: to_uint8 for x in column_names if x.startswith('feature')}\n",
        "    # df = pd.read_csv(file_path, dtype=dtypes, converters=converters)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "functions used for advanced metrics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfUGiCSFI7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea48c4fe-48e2-4462-a023-02b6497264e9"
      },
      "source": [
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "def main():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(155, activation='swish'))\n",
        "    model.add(Dense(105, activation='swish'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(75, activation='swish'))\n",
        "    model.add(Dense(55, activation='swish'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(25, activation='swish'))\n",
        "    model.add(Dense(10, activation='swish'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(5, activation='swish'))\n",
        "    model.add(Dense(1, activation='swish'))\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer=\"adam\",\n",
        "    )\n",
        "    print(\"Loading data...\")\n",
        "    # The training data is used to train your model how to predict the targets.\n",
        "    training_data = read_csv(\"numerai_training_data.csv\")\n",
        "    # The tournament data is the data that Numerai uses to evaluate your model.\n",
        "    tournament_data = read_csv(\"numerai_tournament_data.csv\")\n",
        "\n",
        "    feature_names = [\n",
        "        f for f in training_data.columns if f.startswith(\"feature\")\n",
        "    ]\n",
        "    print(f\"Loaded {len(feature_names)} features\")\n",
        "\n",
        "    # This is the model that generates the included example predictions file.\n",
        "    # Taking too long? Set learning_rate=0.1 and n_estimators=200 to make this run faster.\n",
        "    # Remember to delete example_model.xgb if you change any of the parameters below.\n",
        "\n",
        "    try:\n",
        "        model = keras.models.load_model(MODEL_FILE)\n",
        "        print(\"loaded pre-trained model...\")\n",
        "    except:\n",
        "        print(\"Training model...\")\n",
        "    model.fit(training_data[feature_names], training_data[TARGET_NAME], epochs=800, batch_size=1000, verbose=2)\n",
        "    model.save(MODEL_FILE)\n",
        "\n",
        "    # Generate predictions on both training and tournament data\n",
        "    print(\"Generating predictions...\")\n",
        "    training_data[PREDICTION_NAME] = model.predict(training_data[feature_names])\n",
        "    tournament_data[PREDICTION_NAME] = model.predict(tournament_data[feature_names])\n",
        "\n",
        "    # Check the per-era correlations on the training set (in sample)\n",
        "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std()}\")\n",
        "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
        "\n",
        "    \"\"\"Validation Metrics\"\"\"\n",
        "    # Check the per-era correlations on the validation set (out of sample)\n",
        "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
        "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
        "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
        "          f\"std {validation_correlations.std()}\")\n",
        "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
        "\n",
        "    # Check the \"sharpe\" ratio on the validation set\n",
        "    validation_sharpe = validation_correlations.mean() / validation_correlations.std()\n",
        "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
        "\n",
        "    print(\"checking max drawdown...\")\n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
        "                                                                  min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    print(f\"max drawdown: {max_drawdown}\")\n",
        "\n",
        "    # Check the feature exposure of your validation predictions\n",
        "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
        "                                                             axis=0)\n",
        "    max_feature_exposure = np.max(np.abs(feature_exposures))\n",
        "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
        "\n",
        "    # Check feature neutral mean\n",
        "    print(\"Calculating feature neutral mean...\")\n",
        "    feature_neutral_mean = get_feature_neutral_mean(validation_data)\n",
        "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
        "\n",
        "    # Load example preds to get MMC metrics\n",
        "    example_preds = pd.read_csv(\"example_predictions_target_kazutsugi.csv\").set_index(\"id\")[\"prediction_kazutsugi\"]\n",
        "    validation_example_preds = example_preds.loc[validation_data.index]\n",
        "    validation_data[\"ExamplePreds\"] = validation_example_preds\n",
        "\n",
        "    print(\"calculating MMC stats...\")\n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in validation_data.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[\"ExamplePreds\"])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
        "\n",
        "    print(\n",
        "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
        "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
        "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
        "    )\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(validation_example_preds.rank(pct=True, method=\"first\"),\n",
        "                                          validation_data[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(f\"Corr with example preds: {corr_with_example_preds}\")\n",
        "\n",
        "    # Save predictions as a CSV and upload to https://numer.ai\n",
        "    tournament_data[PREDICTION_NAME].to_csv(TOURNAMENT_NAME + \"_submission.csv\")\n",
        "\n",
        "main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loaded 310 features\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_6_layer_call_and_return_conditional_losses_4224174) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_7_layer_call_and_return_conditional_losses_4224206) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_layer_call_and_return_conditional_losses_4223892) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_7_layer_call_and_return_conditional_losses_4225064) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_4_layer_call_and_return_conditional_losses_4224080) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_2_layer_call_and_return_conditional_losses_4224885) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_1_layer_call_and_return_conditional_losses_4223924) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_layer_call_and_return_conditional_losses_4224808) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_5_layer_call_and_return_conditional_losses_4224987) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_2_layer_call_and_return_conditional_losses_4223986) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_1_layer_call_and_return_conditional_losses_4224833) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224614) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_6_layer_call_and_return_conditional_losses_4225039) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_5_layer_call_and_return_conditional_losses_4224112) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference__wrapped_model_4223871) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_4_layer_call_and_return_conditional_losses_4224962) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_3_layer_call_and_return_conditional_losses_4224018) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_dense_3_layer_call_and_return_conditional_losses_4224910) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:Importing a function (__inference_sequential_layer_call_and_return_conditional_losses_4224718) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "loaded pre-trained model...\n",
            "Epoch 1/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 2/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 3/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 4/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 5/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 6/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 7/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 8/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 9/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 10/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 11/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 12/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 13/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 14/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 15/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 16/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 17/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 18/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 19/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 20/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 21/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 22/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 23/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 24/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 25/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 26/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 27/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 28/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 29/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 30/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 31/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 32/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 33/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 34/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 35/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 36/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 37/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 38/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 39/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 40/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 41/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 42/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 43/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 44/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 45/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 46/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 47/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 48/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 49/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 50/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 51/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 52/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 53/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 54/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 55/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 56/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 57/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 58/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 59/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 60/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 61/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 62/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 63/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 64/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 65/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 66/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 67/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 68/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 69/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 70/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 71/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 72/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 73/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 74/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 75/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 76/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 77/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 78/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 79/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 80/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 81/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 82/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 83/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 84/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 85/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 86/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 87/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 88/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 89/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 90/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 91/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 92/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 93/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 94/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 95/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 96/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 97/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 98/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 99/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 100/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 101/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 102/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 103/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 104/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 105/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 106/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 107/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 108/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 109/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 110/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 111/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 112/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 113/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 114/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 115/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 116/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 117/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 118/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 119/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 120/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 121/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 122/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 123/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 124/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 125/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 126/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 127/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 128/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 129/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 130/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 131/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 132/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 133/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 134/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 135/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 136/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 137/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 138/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 139/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 140/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 141/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 142/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 143/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 144/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 145/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 146/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 147/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 148/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 149/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 150/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 151/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 152/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 153/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 154/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 155/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 156/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 157/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 158/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 159/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 160/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 161/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 162/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 163/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 164/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 165/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 166/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 167/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 168/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 169/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 170/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 171/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 172/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 173/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 174/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 175/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 176/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 177/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 178/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 179/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 180/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 181/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 182/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 183/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 184/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 185/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 186/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 187/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 188/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 189/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 190/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 191/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 192/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 193/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 194/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 195/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 196/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 197/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 198/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 199/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 200/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 201/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 202/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 203/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 204/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 205/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 206/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 207/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 208/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 209/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 210/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 211/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 212/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 213/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 214/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 215/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 216/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 217/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 218/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 219/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 220/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 221/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 222/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 223/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 224/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 225/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 226/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 227/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 228/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 229/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 230/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 231/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 232/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 233/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 234/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 235/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 236/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 237/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 238/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 239/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 240/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 241/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 242/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 243/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 244/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 245/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 246/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 247/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 248/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 249/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 250/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 251/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 252/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 253/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 254/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 255/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 256/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 257/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 258/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 259/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 260/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 261/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 262/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 263/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 264/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 265/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 266/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 267/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 268/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 269/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 270/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 271/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 272/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 273/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 274/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 275/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 276/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 277/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 278/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 279/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 280/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 281/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 282/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 283/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 284/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 285/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 286/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 287/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 288/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 289/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 290/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 291/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 292/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 293/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 294/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 295/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 296/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 297/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 298/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 299/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 300/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 301/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 302/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 303/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 304/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 305/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 306/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 307/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 308/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 309/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 310/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 311/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 312/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 313/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 314/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 315/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 316/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 317/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 318/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 319/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 320/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 321/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 322/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 323/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 324/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 325/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 326/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 327/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 328/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 329/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 330/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 331/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 332/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 333/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 334/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 335/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 336/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 337/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 338/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 339/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 340/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 341/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 342/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 343/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 344/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 345/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 346/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 347/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 348/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 349/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 350/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 351/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 352/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 353/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 354/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 355/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 356/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 357/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 358/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 359/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 360/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 361/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 362/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 363/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 364/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 365/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 366/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 367/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 368/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 369/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 370/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 371/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 372/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 373/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 374/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 375/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 376/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 377/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 378/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 379/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 380/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 381/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 382/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 383/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 384/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 385/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 386/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 387/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 388/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 389/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 390/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 391/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 392/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 393/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 394/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 395/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 396/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 397/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 398/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 399/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 400/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 401/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 402/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 403/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 404/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 405/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 406/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 407/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 408/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 409/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 410/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 411/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 412/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 413/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 414/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 415/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 416/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 417/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 418/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 419/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 420/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 421/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 422/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 423/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 424/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 425/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 426/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 427/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 428/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 429/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 430/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 431/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 432/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 433/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 434/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 435/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 436/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 437/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 438/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 439/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 440/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 441/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 442/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 443/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 444/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 445/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 446/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 447/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 448/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 449/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 450/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 451/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 452/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 453/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 454/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 455/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 456/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 457/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 458/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 459/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 460/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 461/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 462/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 463/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 464/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 465/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 466/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 467/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 468/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 469/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 470/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 471/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 472/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 473/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 474/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 475/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 476/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 477/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 478/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 479/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 480/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 481/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 482/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 483/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 484/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 485/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 486/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 487/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 488/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 489/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 490/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 491/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 492/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 493/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 494/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 495/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 496/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 497/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 498/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 499/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 500/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 501/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 502/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 503/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 504/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 505/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 506/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 507/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 508/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 509/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 510/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 511/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 512/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 513/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 514/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 515/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 516/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 517/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 518/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 519/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 520/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 521/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 522/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 523/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 524/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 525/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 526/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 527/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 528/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 529/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 530/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 531/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 532/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 533/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 534/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 535/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 536/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 537/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 538/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 539/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 540/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 541/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 542/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 543/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 544/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 545/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 546/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 547/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 548/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 549/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 550/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 551/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 552/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 553/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 554/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 555/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 556/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 557/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 558/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 559/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 560/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 561/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 562/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 563/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 564/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 565/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 566/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 567/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 568/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 569/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 570/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 571/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 572/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 573/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 574/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 575/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 576/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 577/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 578/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 579/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 580/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 581/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 582/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 583/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 584/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 585/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 586/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 587/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 588/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 589/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 590/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 591/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 592/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 593/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 594/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 595/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 596/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 597/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 598/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 599/800\n",
            "502/502 - 2s - loss: 0.0895\n",
            "Epoch 600/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 601/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 602/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 603/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 604/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 605/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 606/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 607/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 608/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 609/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 610/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 611/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 612/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 613/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 614/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 615/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 616/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 617/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 618/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 619/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 620/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 621/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 622/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 623/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 624/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 625/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 626/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 627/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 628/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 629/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 630/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 631/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 632/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 633/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 634/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 635/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 636/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 637/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 638/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 639/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 640/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 641/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 642/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 643/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 644/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 645/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 646/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 647/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 648/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 649/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 650/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 651/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 652/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 653/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 654/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 655/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 656/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 657/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 658/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 659/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 660/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 661/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 662/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 663/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 664/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 665/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 666/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 667/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 668/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 669/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 670/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 671/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 672/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 673/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 674/800\n",
            "502/502 - 2s - loss: 0.0887\n",
            "Epoch 675/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 676/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 677/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 678/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 679/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 680/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 681/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 682/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 683/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 684/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 685/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 686/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 687/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 688/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 689/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 690/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 691/800\n",
            "502/502 - 2s - loss: 0.0887\n",
            "Epoch 692/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 693/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 694/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 695/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 696/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 697/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 698/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 699/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 700/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 701/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 702/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 703/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 704/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 705/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 706/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 707/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 708/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 709/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 710/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 711/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 712/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 713/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 714/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 715/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 716/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 717/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 718/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 719/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 720/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 721/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 722/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 723/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 724/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 725/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 726/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 727/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 728/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 729/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 730/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 731/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 732/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 733/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 734/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 735/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 736/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 737/800\n",
            "502/502 - 2s - loss: 0.0894\n",
            "Epoch 738/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 739/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 740/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 741/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 742/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 743/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 744/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 745/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 746/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 747/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 748/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 749/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 750/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 751/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 752/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 753/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 754/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 755/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 756/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 757/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 758/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 759/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 760/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 761/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 762/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 763/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 764/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 765/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 766/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 767/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 768/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 769/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 770/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 771/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 772/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 773/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 774/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 775/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 776/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 777/800\n",
            "502/502 - 2s - loss: 0.0887\n",
            "Epoch 778/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 779/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 780/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 781/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 782/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 783/800\n",
            "502/502 - 2s - loss: 0.0893\n",
            "Epoch 784/800\n",
            "502/502 - 2s - loss: 0.0891\n",
            "Epoch 785/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 786/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 787/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 788/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 789/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 790/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 791/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 792/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 793/800\n",
            "502/502 - 2s - loss: 0.0890\n",
            "Epoch 794/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "Epoch 795/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 796/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 797/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 798/800\n",
            "502/502 - 2s - loss: 0.0892\n",
            "Epoch 799/800\n",
            "502/502 - 2s - loss: 0.0888\n",
            "Epoch 800/800\n",
            "502/502 - 2s - loss: 0.0889\n",
            "INFO:tensorflow:Assets written to: kerasmodel8layersDropout/assets\n",
            "Generating predictions...\n",
            "On training the correlation has mean 0.5668568962179809 and std 0.014378174332802888\n",
            "On training the average per-era payout is 0.25\n",
            "On validation the correlation has mean -0.0005071605407819728 and std 0.016420781318119457\n",
            "On validation the average per-era payout is -0.0005071605407819728\n",
            "Validation Sharpe: -0.03088528681776842\n",
            "checking max drawdown...\n",
            "max drawdown: -0.05703881152281942\n",
            "Max Feature Exposure: 0.07005498168322337\n",
            "Calculating feature neutral mean...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Neutral Mean is -0.0013956281940177362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "calculating MMC stats...\n",
            "MMC Mean: -0.005541962566278054\n",
            "Corr Plus MMC Sharpe:-0.17070970859944598\n",
            "Corr Plus MMC Diff:-0.13982442178167756\n",
            "Corr with example preds: 0.1457835707339683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scpEq4YKzn1E"
      },
      "source": [
        "8 x 800 epochs, steady increase until now. CV of previous ; 0.005"
      ]
    }
  ]
}